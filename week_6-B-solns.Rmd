---
title: "Week 6, Day 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)

# Build an urn with 500 red beads, 490 blue beads, and 10 yellow beads. The urn
# should have two variables: ID and color. Mix up the beads in the urn randomly.
# That is, we don't want all the red beads to have ID 1 through 500 and so on.
# Sort the urn by ID.

beads <- c(rep("red", 500), rep("blue", 490), rep("yellow", 10))

urn <- tibble(ID = 1:1000,
              color = sample(beads))

# test to see of the randomness of the urn was working

# urn %>% 
#   slice(1:10)

# Using the book, where a very similar example is given, is highly recommended.
# But, this example is trickier than the book because you have to mix up the
# beads before you assign the `ID` values. If you don't tell sample() how many
# you want, it just gives you back everything, reordered. Try `sample(letters)`
# to see. (Note that `letters` is a built in object in R.)

# sample(letters)
              
```

We are learning about sampling this week. We are taking the theory of Chapter 5 and applying it in a not-too-toyish example. There is a single, true, unknown parameter. What is your posterior distribution for that parameter? Once you have that posterior, how can you use it to make forecasts about the future? 



## Scene 1

**Prompt:** As in the book, we will be calculating our posterior distribution of the number of red beads in the urn. Assume that we know that there are 1,000 beads in the urn, all either red, blue or yellow. Create an unnormalized joint distribution of the set of models we are considering and the possible results of our experiment. In this example, we are using a paddle of size 25. (You may want to review the meaning of an unnormalized joint distribution from Chapter 5.) Plot that distribution. It should look very similar to the first plot in Section 6.6.1.

Interpret the meaning of the graphic.

Hint: Review the use of `rbinom()`, especially the `n`, `size` and `prob` arguments, from Chapter 2. Start by creating a tibble with one row. In that row, we assume that there are truly 100 red beads. Draw, in that case, from `rbinom()`, with the appropriate arguments, to see how many red beads we might draw in that case. Call that variable `numb_red`. But that is just one draw! Now make a tibble with two such rows. `map_int()` is a nice tool. See the pattern? We just need a tibble with lots of rows, each one indicating both the "true" number of red beads, and then the number we draw with our paddle. But each value of the true number of red beads needs more than one row, otherwise we won't have enough data to describe the full joint distribution.

```{r sc1}
# It can be useful to set up the key variables at the top of a block of code, if
# only to make it easier to keep track of things. This is especially important,
# which is case here for urn_size, if the variable is used in multiple places in
# the code.

urn_size <- nrow(urn)
paddle_size <- 25
reps <- 1000

# Make a tibble with a million rows or so.

x <- tibble(numb_red = rep(seq(0, urn_size, 1), reps)) %>%
  mutate(red_paddle = map_int(numb_red, ~ rbinom(n = 1, 
                                                size = paddle_size, 
                                                prob = ./urn_size)))

x %>%
  ggplot(aes(y = numb_red, x = red_paddle)) +
    geom_point(alpha = 0.01) +
    labs(title = "Joint Distribution of Red Beads in Paddle and in Urn",
         x = "Number of Red Beads in Paddle",
         y = "Number of Red Beads in Urn")
```


**Comment:** There is very little code here, but it can be tricky to create. Key insight: Assume that there are 500 red beads out of 1,000 in the urn. How many might get pulled out with a 25 paddle? This was hard enough that I left extensive hints.

It does not matter how many other colors there are in the urn. All that matters is how many red there are in the sample. 

Key lesson is interpreting that joint distribution. What does it mean? What is p(models, data)? I find it useful to look at a single row. All the points in a row, say 250, are possible experimental results if, in fact there are 250 red beads in the urn.

Might also consider geom_jitter(), although with zero height jitter.

## Scene 2

**Prompt:** Using the joint distribution we calculated in Scene 5 as input, calculate your posterior probability density for the number of red beads in the urn, given that that 10 red were sampled in our paddle of 25.

```{r sc2}
post <- x %>% 
  filter(red_paddle == 10) %>% 
  group_by(numb_red) %>% 
  summarize(total = n(), .groups = "drop") %>%
  mutate(probs = total/sum(total)) %>% 
  select(-total)

post %>% 
  ggplot(aes(x = numb_red, y = probs)) +
    geom_col() +
    labs(title = "Posterior Probability Distribution of Red Beads in Urn",
         x = "Number of Red Beads",
         y = "Probability")


```



**Comment:** If we interpreted the previous graphic by thinking about rows, this question is asking us to pick out a column, specifically the column of data at 10 red beads. Once we filter to just include that column, we just need to normalized and, viola!, we have a posterior distribution. It is very handy to have created the `post` object since we will use that for Scene 5.

This is our first act of inference! We start with something that is real but unknown to us: the number of red beads in the urn. We gather some data, in this case a sample of 50, in which 20 of the beads are red. We use this data, and a statistical model, to create a posterior probability distribution for the unknown value.



## Scene 3

**Prompt:** With our posterior probability distribution, we can now forecast outcomes which we have not yet seen. For example, what is the probability of getting more than 3 reds if we sample 20 from the urn, given our posterior? (That is, we are pretending that we don't know the number of red beads in the urn.)


```{r sc3}
post %>% 
  mutate(new_reds = map(numb_red, 
                        ~ rbinom(n = 100, 
                                 size = 20,
                                 prob = ./1000))) %>% 
  unnest(new_reds) %>% 
  mutate(res = ifelse(new_reds > 3, TRUE, FALSE)) %>% 
  summarise(final = weighted.mean(res, probs))
  
```


**Comment:** We are doing all these exercises "by hand" in order to help students develop their intuition. What is a posterior distribution exactly and how do we use it? In Chapter 7 and after, we will just let R worry about the messy details. 

The answer is about 96%, which might seem low. After all, if our best guess is 40% then the expected number of red beads is about 8. 

There are a couple of tricks in the code:

* The `prob` argument to `rbinom()` varies each time we call it because each row, by assumption, is based on a possibility different assumption about the number of red beads in the urn.  

* `n` could be 1 or 100 or 10,000. This is just a simple way of saying: Do this experiment more than one time for each row.

* The annoyance of n = 100 is that it makes `new_reds` a list column with a vector, and vectors are hard to work with. Look at the tibble at this stage. But then `unnest()`, which is a magical command, just expands that list-column so that each element gets its own row. Very convenient!

* We can't just look at the proportion of results with more than three red beads. We have to weight these results by the posterior probability. The intuition behind that fact is subtle but important.


At the same time, I would not worry too much about these tricks. We don't use them that often. The intuition behind using the posterior is the most important thing. We have to take account of the uncertainty associated with our parameter estimates.

Conceptually, consider all possible models --- in this case all possible values for the number of red beads in the urn, from 0 to 1,000. Consider, conditional on each of them being true, the probability of getting more than 3 red beads if you draw 20. Weight those probabilities by your posterior, and then take the sum. (That is what a weighted mean is.) And that is your answer!


